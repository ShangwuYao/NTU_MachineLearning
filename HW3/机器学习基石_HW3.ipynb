{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8250003566832635\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "u, v = 0, 0\n",
    "for i in range(5):\n",
    "    uvalue = exp(u) + v * exp(u * v) + 2 * u - 2 * v - 3\n",
    "    vvalue = 2 * exp(2 * v) + u * exp(u * v) - 2 * u + 4 * v - 2\n",
    "    u, v = u - 0.01 * uvalue, v - 0.01 * vvalue\n",
    "\n",
    "# print (u)\n",
    "# print (v)\n",
    "E = exp(u) + exp(2 * v) + exp(u * v) + pow(u, 2) - 2 * u * v + 2 * pow(v, 2) - 3 * u - 2 * v\n",
    "print (E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.36082334564\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from numpy import *\n",
    "u, v = 0, 0\n",
    "for i in range(5):\n",
    "    H = mat([[exp(u) + pow(v, 2) * exp(u * v) + 2, exp(u * v) + u * v * exp(u * v) - 2],[exp(u * v) + u * v * exp(u * v) - 2, 4 * exp(2 * v) + pow(u, 2) * exp(u * v) + 4]])\n",
    "    inverse = H.I\n",
    "    descent = [exp(u) + v * exp(u * v) + 2 * u - 2 * v - 3, 2 * exp(2 * v) + u * exp(u * v) - 2 * u + 4 * v - 2]\n",
    "    matrix = [u, v] - inner(inverse, descent)\n",
    "    u = matrix[0,0]\n",
    "    v = matrix[0,1]\n",
    "\n",
    "E = exp(u) + exp(2 * v) + exp(u * v) + pow(u, 2) - 2 * u * v + 2 * pow(v, 2) - 3 * u - 2 * v\n",
    "print (E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1234499999999999\n",
      "0.12287999999999985\n"
     ]
    }
   ],
   "source": [
    "# question 13~15\n",
    "# linear regression\n",
    "from random import uniform\n",
    "from numpy import *\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0 : return 1\n",
    "    else: return -1\n",
    "    \n",
    "def noise():\n",
    "    x = random.uniform(0,1)\n",
    "    if x > 0.1:\n",
    "        return 1\n",
    "    else :\n",
    "        return -1\n",
    "    \n",
    "def generate_X():\n",
    "    X = []\n",
    "    for i in range(DATA_NUMBER):\n",
    "        x0 = 1\n",
    "        x1 = random.uniform(-1, 1)\n",
    "        x2 = random.uniform(-1, 1)\n",
    "        X.append([x0, x1, x2])\n",
    "    return X\n",
    "\n",
    "def generate_Y(X):\n",
    "    Y = []\n",
    "    for x in X:\n",
    "        y = sign(pow(x[1], 2) + pow(x[2], 2) - 0.6) * noise()\n",
    "        Y.append(y)\n",
    "    return Y\n",
    "\n",
    "def feature_transform(X):\n",
    "    Z = []\n",
    "    for x in X:\n",
    "        # 1, x1, x2, x1x2, x1 ^2, x2 ^2\n",
    "        w0 = 1\n",
    "        w1 = x[1]\n",
    "        w2 = x[2]\n",
    "        w3 = x[1] * x[2]\n",
    "        w4 = power(x[1], 2)\n",
    "        w5 = power(x[2], 2)\n",
    "        Z.append([w0,w1,w2,w3,w4,w5])\n",
    "    return Z\n",
    "\n",
    "def get_W(X, Y):\n",
    "    W = linalg.inv(transpose(X).dot(array(X))).dot(transpose(X)).dot(Y)\n",
    "    return W\n",
    "    \n",
    "def test(X, Y, W):\n",
    "    train_Y = array(W).dot(transpose(X))\n",
    "    count = 0\n",
    "    for i in range(len(Y)):\n",
    "        if sign(train_Y[i]) != Y[i]:\n",
    "            count += 1\n",
    "    return count / len(Y)\n",
    "\n",
    "DATA_NUMBER = 1000\n",
    "LOOP_TIMES = 1000\n",
    "\n",
    "def main():\n",
    "    sum_W = [0,0,0,0,0,0]\n",
    "    Ein_sum = 0\n",
    "    for i in range(LOOP_TIMES):\n",
    "        X = generate_X()\n",
    "        Y = generate_Y(X) # add noise\n",
    "        # question 14\n",
    "        Z = feature_transform(X)\n",
    "        W = get_W(Z, Y)\n",
    "        sum_W = sum_W + W\n",
    "        Ein_temp = test(Z, Y, W)\n",
    "        Ein_sum += Ein_temp\n",
    "    Ein_aver = Ein_sum / LOOP_TIMES\n",
    "    aver_W = sum_W / LOOP_TIMES\n",
    "    \n",
    "    #question 15\n",
    "    Eout_sum = 0\n",
    "    for i in range(LOOP_TIMES):\n",
    "        test_X = generate_X()\n",
    "        test_Y = generate_Y(test_X)\n",
    "        test_Z = feature_transform(test_X)\n",
    "        Eout_sum += test(test_Z, test_Y, aver_W)\n",
    "    Eout_aver = Eout_sum / LOOP_TIMES\n",
    "    print (Ein_aver)\n",
    "    print(Eout_aver)\n",
    "    # print(aver_W)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# question 18~20\n",
    "# Logistic Regression\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sign(x):\n",
    "    if x > 0 : return 1\n",
    "    else : return -1\n",
    "\n",
    "def read_data(input_file):\n",
    "    f = open(input_file)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in f:\n",
    "        strs = line.split()\n",
    "        x = [1] + [float(v) for v in strs[:-1]]\n",
    "        y = int(strs[-1])\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + exp(-s))\n",
    "    \n",
    "def calculate_gratitude_Ein(X, Y, W):\n",
    "    sum = [0 for i in range(len(X[0]))]\n",
    "    for i in range(len(Y)):\n",
    "        sum += sigmoid(-Y[i] * array(W).dot(np.transpose(X[i]))) * (-Y[i] * array(X[i]))\n",
    "    return sum / len(Y)\n",
    "    \n",
    "def batchGradient(X, Y):\n",
    "    W = [0 for i in range(len(X[0]))]\n",
    "    for k in range(LOOP_TIMES):\n",
    "        for i in range(len(Y)):\n",
    "            if (sign(array(W).dot(np.transpose(X[i]))) != Y[i]):\n",
    "                gratitude_Ein = calculate_gratitude_Ein(X, Y, W)\n",
    "                W = W - eta * gratitude_Ein\n",
    "    return W\n",
    "\n",
    "def stochasticGradient(X, Y):\n",
    "    W = [0 for i in range(len(X[0]))]\n",
    "    for k in range(LOOP_TIMES):\n",
    "        i = random.randrange(len(Y))\n",
    "        W = W + eta * sigmoid(- Y[i] * np.array(W).dot(np.transpose(np.array(X[i])))) * Y[i] * np.array(X[i])\n",
    "    return W\n",
    "            \n",
    "def test(X, Y, W):\n",
    "    count = 0\n",
    "    for i in range(len(Y)):\n",
    "        if sign(array(W).dot(np.transpose(X[i]))) != Y[i]:\n",
    "            count += 1\n",
    "    return count / len(Y)\n",
    "\n",
    "TRAIN_DATA = 'train_data.dat'\n",
    "TEST_DATA = 'test_data.dat'\n",
    "LOOP_TIMES = 40000\n",
    "eta = 0.01\n",
    "\n",
    "def main():\n",
    "    train_X, train_Y = read_data(TRAIN_DATA)\n",
    "    # question 18\n",
    "    # W = batchGradient(train_X, train_Y)\n",
    "    # question 19\n",
    "    W = stochasticGradient(train_X, train_Y)\n",
    "    Ein = test(train_X, train_Y, W)\n",
    "    test_X, test_Y = read_data(TEST_DATA)\n",
    "    Eout = test(test_X, test_Y, W)\n",
    "    # print(Ein)\n",
    "    # print(Eout)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
